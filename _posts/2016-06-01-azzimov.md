---
layout: post
title:  "Learnings from a DevOps Hackfest with Azzimov"
author: "Julien Stroheker & William Buchwalter"
author-link: "#"
#author-image: "{{ site.baseurl }}/images/authors/photo.jpg"
date:   2016-06-01
categories: DevOps
color: "blue"
#image: "{{ site.baseurl }}/images/azzimov1.jpg" #should be ~350px tall
excerpt: This article is aimed a sharing the learnings from a DevOps Hackfest with Azzimov.
---

## Learning from Hackfest with *Azzimov* ##

Azzimov teamed up with Microsoft during a few days to implement DevOps practices. The main focus was put on increasing quality and reliability. We will describe in this report the steps we have taken and their results.

Among other things we talked about:
* Unit Testing
* Continuous Integration with Jenkins
* Continuous Deployment
* Integration Testing

The hacking team included:

* Kevork Aghazarian
* Hernán Gioffré
* Karl Gagnon
* Yanick Plamondon
* Andrea Castano
* Prasad Perera
* Yassine Zeroual
* William Buchwalter (Microsoft)
* Julien Stroheker (Microsoft)

## Customer Profile ##
Azzimov is a startup based in Montréal, Canada, which delivers a virtual shopping mall solution leveraging machine learning.

## Problem Statement ##

Azzimov was having quality issues when deploying new releases: Due to the lack of automated test, they had no way to ensure a certain quality of their product before pushing it in production. This is a very usual situation in small to medium companies.
As a secondary objective, Azzimov also wanted to reduce the time to market of new features.
One of the main pain of Azzimov is the lack of tooling around DevOps Practices, they clearly identified their hope to improve this point during the hackfest.

### Current Architecture ###

![Architecture overview picture](/images/AzzimovArchi.jpg)

Azzimov's architecutre is very granular with each component being isolated. This is a very good practice that makes changing any one of the components much easier (and that was great for our hackfest :) ).
It would also greatly simplify moving to a container based architecture, but that's for another day.

For the purpose of this hackfest, we decided to hack on their most complicated component: Trinity Gateway.
If we managed to do something meaningful there, then Azzimov's team was confident they could implement similar practices for their other components.

Trinity Gateway is a Java based service that acts as an orchestrator for all other services of the solution.
It is build with Maven 2.


## Solutions, Steps, and Delivery ##

The first day was dedicated to value stream mapping. This process allows to quickly capture waste and issues in the workflow from ideation to production.

![VSM Picture here](/images/AzzimovVSM.jpg)

This exercise allowed us to quickly understand the main issue: There is no real quality check in the process except some sparse manual testing. This allows defects to be passed downstream to production: releasing is thus very risky.

We also found that the mean lead time from the planning of a feature to it's release in production is around 5 weeks. This lead time was probably somewhat optimistic, but not too far from the truth.

ultimately, the goal of DevOps is to reduce this lead time as much as possible to allow rapid feedback and iteration, but in order to do that, we need a way to be confident in the quality of our product.

For these reasons we decided to focus on continuous integration and continous deployment of new changes into a staging environment first. Done properly, This would allow to catch issues much faster than today, and most importantly, to spot them before they impact a customer.
This are the basic building blocs of DevOps that can be built upon later on.

Because Azzimov's team already had some experience with Jenkins, we decided to go ahead and use it.
Because we love automation, and we don't want to setup anything manually, we deployed a new Jenkins instance on Azure using an [ARM template](https://github.com/Azure/azure-quickstart-templates/tree/master/jenkins-on-ubuntu) that can be parametrized to setup any number of nodes.

Once Jenkins setup, we split the work between two teams: One team would work on continuous integration, the other would work on automating deployment on a staging environment.

### Continuous Integration

One of the main pain that we identified during the Value Stream Mapping was the lack of tooling to help Azzimov implement some DevOps Practices.

From the build to the release, each steps was manual and done by different people in the company without any official process.
The build process (using Maven 2) was executed manually from a developer machine.

During the Hackfest we explained the concept of  Continuous Integration and the possibility to build the code at each commit in their GitHub repository, as well as the impact of adding units testing and running them at build time.

We decided to automated those tasks and created a Pipeline project in Jenkins 2.0.

We identified four stages:
* Checkout Sources : Classic git clone of the repository
* Build : Building the .Jar file with Maven.
* Unit tests : Run a shell command to execute the JUnit tests in the project
* Publish Artifacts : Upload the .Jar file in an Azure Blog Storage, ready to be deployed

Here is the code of this pipeline in Jenkins :

{% highlight groovy %}
node {
   ws('/var/lib/jenkins/workspace/trinity') {
    // Get the maven tool.
    def mvnHome = '/usr/share/maven'

    stage 'Checkout Sources'
      git branch: 'feature/jenkin_integration', credentialsId: 'XXXXX-XXXX-XXXX-XXXX8-XXXXXXXXXX', url: 'https://github.com/XXXXX/YYYYY'

    stage 'Build'
      sh "${mvnHome}/bin/mvn clean install -DskipTests"

    stage 'Unit Tests'
      sh "${mvnHome}/bin/mvn test"
      step([$class: 'JUnitResultArchiver', testResults: '**/target/surefire-reports/TEST-*.xml'])

    //publish the war file to azure, so it can be used later to setup a new VM  
    stage 'Publish Artifacts'
      sh "azure storage blob upload -q -a \"azziartifacts\" -k \"XXXXXXXXXXXXXXXXXXX" \"target/SearchGateway.war\" \"trinity\""
   }
{% endhighlight %}

![Pipeline Jenkins](/images/AzzimovPipeline4First.jpg)

The next step was to deploy this artifact (.Jar) automatically in a new environment to run the integration tests.

### Continuous Deployment to staging



![Pipeline Jenkins](/images/AzzimovPipeline.jpg)

### Functional testing
Since we went faster than expected, we then decided to push ahead with our objective of improving quality.
One of the issue, is that their existing code base as very few unit tests. As any developer will tell you, adding unit test to existing code, that was written by someone else is a very hard task. If the codebase is somewhat large this can be a huge time investment.

While it is probably always worth going back and adding unit test to critical pieces of the application, functional tests will offer a better return on investment on the short term. By testing the whole stack, we can quickly make sure everything is working as expected with minimal efforts.

During the value stream mapping we identified that the team did some manual sanity checks using curl before going into production.
We decided to create a new repository containing functional tests for all the different services, and capture the manual tests as a first step.

We decided to use Node.js for functional testing for multiple reasons:
  * Easy to pick up, and some developer already have experience with it.
  * With a library such as mocha we can easily export test results in an xUnit format to display them in Jenkins
  * Libraries such as phantomjs will make UI testing easier on a build agent.

Here is an example of what this new tests look like:

{% highlight javascript %}
require('isomorphic-fetch');
var expect = require('expect');

describe('Connection tests', function() {
this.timeout(5000);  
  it('Initiate Session and Test MySQl connection', function(done) {
    fetch('http://someinternalurl.com:8080/')
      .then(res => res.json())
      .then(res => {
        expect(res.status).toEqual(1);
        done();
      })
      .catch(err => done(err));    
  })
});
{% endhighlight %}

We then hooked this tests into the Jenkinsfile so that they are executed once the environment finishes deploying.

{% highlight groovy %}
ws('/var/lib/jenkins/workspace/trinity-tests') {
    stage 'Checkout Functional Tests'
      git branch: 'master', url: 'https://github.com/xxxx/xxxx'

    stage 'Install dependencies'
      sh 'npm i'

    stage 'Gateway Tests'
      sh 'npm run gateway'

    stage 'Mall Tests'
      sh 'npm run mall'

    stage 'Archiving Test Results'
      //This step archive the xml file containing testresults to display them in jenkins UI
      step([$class: 'JUnitResultArchiver', testResults: '*.xml'])
  }
{% endhighlight %}

## Conclusion ##

While we can only do so much in 5 days, continuous integration, the addition of critical unit tests, and autmated integration tests should quickly make a difference in the stability of the application when releasing a new update.

In the mean time, continuous deployment should help reduce the lead time, by letting developers seemlessly verify their changes in an environment.  

## General lessons ##
Bulleted list of insights the Hackfest team came away with

What can be applied or reused in other environments or other customers ?

## Resources ##
Links to additional resource (documentation, blog posts, github repos, ...)
