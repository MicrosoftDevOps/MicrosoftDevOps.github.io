---
layout: post
title:  "Learnings from a DevOps Hackfest with Azzimov"
author: "Julien Stroheker & William Buchwalter"
author-link: "#"
#author-image: "{{ site.baseurl }}/images/authors/photo.jpg"
date:   2016-06-01
categories: DevOps
color: "blue"
#image: "{{ site.baseurl }}/images/azzimov1.jpg" #should be ~350px tall
excerpt: This article is aimed a sharing the learnings from a DevOps Hackfest with Azzimov.
---

## Learning from Hackfest with *Azzimov* ##

Azzimov teamed up with Microsoft during a few days to implement DevOps practices into their processes. We will describe in this report the steps we have taken and their results.

Among other things we talked about:
* Unit Testing
* Continuous Integration with Jenkins
* Continuous Deployment
* Integration Testing

The hacking team included:

* Kevork Aghazarian
* Hernán Gioffré
* Karl Gagnon
* Yanick Plamondon
* Andrea Castano
* Prasad Perera
* Yassine Zeroual
* William Buchwalter (Microsoft)
* Julien Stroheker (Microsoft)

## Customer Profile ##
Azzimov is a startup based in Montréal, Canada, which delivers a virtual shopping mall solution leveraging machine learning.
The main focus of this hackfest was increasing quality and reliability of the product and it's deployments.

## Problem Statement ##

Azzimov was having quality issues when deploying new releases: Due to the lack of automated test, they had no way to ensure a certain quality of their product before pushing it in production. This is a very usual situation in small to medium companies.
As a secondary objective, Azzimov also wanted to reduce the time to market of new features.
One of the main pain of Azzimov is the lack of tooling around DevOps Practices, they clearly identified their hope to improve this point during the hackfest.

### Current Architecture ###

![Architecture overview](/images/AzzimovArchi.jpg)

Azzimov's architecutre is very granular with each component being isolated. This is a very good practice that makes changing any one of the components much easier (and that was great for our hackfest :) ).
It would also greatly simplify moving to a container based architecture, but that's for another day.

For the purpose of this hackfest, we decided to hack on their most complicated component: Trinity Gateway.
If we managed to do something meaningful there, then Azzimov's team was confident they could implement similar practices for their other components.

Trinity Gateway is a Java based service that acts as an orchestrator for all other services of the solution.
It is build with Maven 2.


## Solutions, Steps, and Delivery ##

The first day was dedicated to value stream mapping. This process allows to quickly capture waste and issues in the workflow from ideation to production.

![Value Stream Mapping](/images/AzzimovVSM.jpg)

This exercise allowed us to quickly understand the main issue: There is no quality check in the process except some sparse manual testing. This allows defects to be passed downstream to production: releasing is thus very risky.

We also found that the mean lead time from the planning of a feature to it's release in production is around 5 weeks. This lead time was probably somewhat optimistic, but not too far from the truth.

ultimately, the goal of DevOps is to reduce this lead time as much as possible to allow rapid feedback and iteration, but in order to do that, we need a way to be confident in the quality of our product.

For these reasons we decided to focus on continuous integration and continous deployment of new changes into a staging environment first. Done properly, This would allow to catch issues much faster than today, and most importantly, to spot them before they impact a customer.
This are the basic building blocs of DevOps that can be built upon later on.

Because Azzimov's team already had some experience with Jenkins, we decided to go ahead and use it.
Because we love automation, and we don't want to setup anything manually, we deployed a new Jenkins instance on Azure using an [ARM template](https://github.com/Azure/azure-quickstart-templates/tree/master/jenkins-on-ubuntu) that can be parametrized to setup any number of nodes.

Once Jenkins setup, we split the work between two teams: One team would work on continuous integration, the other would work on automating deployment on a staging environment.

### Continuous Integration

One of the main pain that we identified during the Value Stream Mapping was the lack of tooling to help Azzimov implement some DevOps Practices.

From the build to the release, each steps was manual and done by different people in the company without any official process.
The build process (using Maven 2) was executed manually from a developer machine.

During the Hackfest we explained the concept of  Continuous Integration and the possibility to build the code at each commit in their GitHub repository, as well as the impact of adding units testing and running them at build time.

We decided to automated those tasks and created a Pipeline project in Jenkins 2.0.

We identified four stages:

* **Checkout Sources** : Classic git clone of the repository
* **Build** : Building the .Jar file with Maven.
* **Unit tests** : Run a shell command to execute the JUnit tests in the project
* **Publish Artifacts** : Upload the .Jar file in an Azure Blog Storage, ready to be deployed

Here is the code of this pipeline in Jenkins :

{% highlight groovy %}
node {
   ws('/var/lib/jenkins/workspace/trinity') {
    // Get the maven tool.
    def mvnHome = '/usr/share/maven'

    stage 'Checkout Sources'
      git branch: 'feature/jenkin_integration', credentialsId: 'XXXXX-XXXX-XXXX-XXXX8-XXXXXXXXXX', url: 'https://github.com/XXXXX/YYYYY'

    stage 'Build'
      sh "${mvnHome}/bin/mvn clean install -DskipTests"

    stage 'Unit Tests'
      sh "${mvnHome}/bin/mvn test"
      step([$class: 'JUnitResultArchiver', testResults: '**/target/surefire-reports/TEST-*.xml'])

    //publish the war file to azure, so it can be used later to setup a new VM  
    stage 'Publish Artifacts'
      sh "azure storage blob upload -q -a \"azziartifacts\" -k \"XXXXXXXXXXXXXXXXXXX" \"target/SearchGateway.war\" \"trinity\""
   }
{% endhighlight %}

![Pipeline Jenkins](/images/AzzimovPipeline4First.jpg)

The next step was to deploy this artifact (.Jar) automatically in a new environment to run the integration tests.

### Continuous Deployment to staging

The idea was to automatically deploy the artifact previously built in an environment, ready to be tested.

For this part, we introduced the concepts of Infrastructure as Code and Configuration Management on Microsoft Azure with  Azzimov's team.

First, we had to define the needs for the application :

* An ubuntu environment
* Java Development Kit installed (JDK)
* Tomcat binaries
* Download the artifact from the Azure Blob Storage
* Lunch Tomcat on the port 8080 with the Jar file

With an ARM template, we automatized the deployment of one Virtual Machine running Ubuntu on Azure. When this deployment was done, we added a custom extension written on Shell to run our custom settings.

One interesting countermeasure that we had to find, was the way to automatically accept the License Agreement from Java :

![Pipeline Jenkins](/images/AzzimovJavaPrompt.png)

We used the '**echo debconf**' command to remediate to it (See the next snippet)

Here is a piece of code of the shell script that we push as a custom extension in the VM :

{% highlight shell %}
...

sudo add-apt-repository -y ppa:webupd8team/java
sudo apt-get -y update
echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections
echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections
sudo apt-get -y install oracle-java7-installer
sudo apt-get -y install oracle-java7-set-default

...
{% endhighlight %}

So now that we have one json file containing our Infrastructure as code part and one shell script containing our configuration part we are ready to execute it every time that we want.

So we included those next steps inside our Jenkins pipeline :

* **Checkout ARM Template** : Classic git clone of the repository containing the ARM template + Shell Script
* **Upload Deployment Script**  : Upload the shell file in an Azure Blog Storage, ready to be executed as a custom extension
* **Creating Resource Group** : Execute an Azure CLI command to create a resource group on Azure where we will deploy our environment
* **Deploy integration env** : Execute an Azure CLI command to deploy our environment thank to our ARM template
* **Installing trinity** : Add a custom extension on the virtual machine previously created to execute our shell script
* **Send SMS** : Send a text message to one of the OPS people to acknowledge the deployment using the Trello.com API


![Pipeline Jenkins](/images/AzzimovPipeline.jpg)

### Functional testing
We then decided to push ahead with our objective of improving quality.
One of the issue is that their existing code base has very few unit tests. As any developer will tell you, adding unit test to an existing code base that was written by someone else is a very hard task. If the codebase is somewhat large this can be a huge time investment.

While it is probably always worth going back and adding unit test to critical pieces of the application, functional tests will offer a better return on investment on the short term. By testing the whole stack, we can quickly make sure everything is working as expected before a release with minimal efforts.  

During the value stream mapping we identified that the team did some manual sanity checks using curl before going into production.
We decided to create a new repository containing functional tests for all the different services, and capture the manual tests as a first step.

We decided to use Node.js for functional testing for multiple reasons:
  * Easy to pick up, and some developer already have experience with it.
  * With a library such as mocha we can easily export test results in an xUnit format to display them in Jenkins
  * Libraries such as phantomjs will make UI testing easier on a build agent.

Here is an example of what this new tests look like:

{% highlight javascript %}
require('isomorphic-fetch');
var expect = require('expect');

describe('Connection tests', function() {
this.timeout(5000);  
  it('Initiate Session and Test MySQl connection', function(done) {
    fetch('http://someinternalurl.com:8080/')
      .then(res => res.json())
      .then(res => {
        expect(res.status).toEqual(1);
        done();
      })
      .catch(err => done(err));    
  })
});
{% endhighlight %}

We then hooked this tests into the Jenkinsfile so that they are executed once the environment finishes deploying.

{% highlight groovy %}
ws('/var/lib/jenkins/workspace/trinity-tests') {
    stage 'Checkout Functional Tests'
      git branch: 'master', url: 'https://github.com/xxxx/xxxx'

    stage 'Install dependencies'
      sh 'npm i'

    stage 'Gateway Tests'
      sh 'npm run gateway'

    stage 'Mall Tests'
      sh 'npm run mall'

    stage 'Archiving Test Results'
      //This step archive the xml file containing testresults to display them in jenkins UI
      step([$class: 'JUnitResultArchiver', testResults: '*.xml'])
  }
{% endhighlight %}

## Conclusion ##

While we can only do so much in 5 days, continuous integration, the addition of critical unit tests, and automated integration tests should quickly make a difference in the stability of the application when releasing a new update.

In the mean time, continuous deployment should help reduce the lead time, by letting developers seemlessly verify their changes in an environment.  

## General lessons ##
Bulleted list of insights the Hackfest team came away with

What can be applied or reused in other environments or other customers ?

## Resources ##
Links to additional resource (documentation, blog posts, github repos, ...)
