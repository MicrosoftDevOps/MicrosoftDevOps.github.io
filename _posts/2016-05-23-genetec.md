---
layout: post
title:  "Learnings from a DevOps Hackfest with Genetec"
author: "Julien Stroheker & William Buchwalter"
author-link: "#"
#author-image: "{{ site.baseurl }}/images/authors/photo.jpg"
date:   2016-05-23
categories: DevOps
color: "blue"
#image: "{{ site.baseurl }}/images/genetec1.jpg" #should be ~350px tall
excerpt: This article is aimed a sharing the learnings from the DevOps Hackfest with Genetec.
---
![Figure 0: Genetec Logo](/images/genetec0.png)
{: .center}

## Learning from Hackfest with Genetec ##

Genetec teamed up with Microsoft for a few days to bring DevOps practices and mindset to Stratocast, a cloud based video monitoring system.
During this short event, we touched a lot of concept, among which:
- Continuous Integration/Continuous Deployment
- Tests: Integration tests, UI tests and more.
- Cross departments collaboration.

### Hack Team
**Genetec**

* Simon le bourdais Cabana - Product Owner
* Racha Karout - Test Engineer Team Lead
* Gregoire Wolf - System Test Team Leader
* Paul Evans - Backend System Test Champion
* Parvez-Alam - Lead Cloud Ops Eng.
* Sébastien Nadeau - Backend Developer Team Lead
* David Chapdelaine - Backend Developer
* Francisco Davila - Backend Developer
* Yakim Mandjee - Backend Developer
* Patterson Deshomme - Backend Developer
* Louis-Phillipe Jetté - Backend Developer

**Microsoft**

* David Tesar [(@dtzar)](https://twitter.com/dtzar) - Sr. DevOps Technical Evangelist
* Julien Stroheker [(@Ju_stroh)](https://twitter.com/Ju_Stroh) - DevOps Technical Evangelist
* William Buchwalter [(@wbuchw)](https://twitter.com/wbuchw) - DevOps Technical Evangelist

## Customer Profile ##

### Genetec ###

Genetec is a global provider of IP video surveillance, access control and license plate recognition solutions unified in a single platform, called **Security Center**. They work with partners on six continents to help provide safer, more secure environments for small to medium-sized and enterprise-class businesses in over 80 countries.

Genetec built the **Security Center** solution platform in Microsoft Azure cloud.
Two employees of Genetec were present at our DevOps Hackathon in Montreal in March; during this 2-day training event, they learned how to embrace a DevOps mindset as we introduced some DevOps concepts with them.

*Figure 1: Value Stream Map of Genetec application*

![Figure 1: Value Stream Map of Genetec application](/images/genetec1.jpg)

Three months after the Hackathon, Genetec and Microsoft organized the Hackfest on the previous picture together with the goal to improve and implement some DevOps practices in their current production environment.

### Product ###
During our Hackfest, we worked mainly on the backend part of the Security Center solution.
It is interesting to know that Genetec launched an offer called Stratocast, which is a camera security center 100% cloud based solution without any physical devices to install on premise. The idea is to offer the “Plug And Play” experience with cameras, without care for Firewalls or networking aspects for example. The target of this offer is for the small/medium size enterprise.
Behind the scenes, Stratocast is using the same binaries of the enterprise wise solution: Security Center but offered in SaaS mode thanks to Azure.
In terms of architecture on Azure, the application is using:

* Cloud Services with 4/5 workers role
* Azure Queue
* Azure SQL Databases
* Storages Account

*Figure 2: Architecture Overview*
![Figure 2: Architecture Overview](/images/genetec2.jpg)

We mainly focused our technical efforts on the part within the blue square on the previous image, which represents the backend solution, using cloud services architecture on Microsoft Azure.

This current piece of code is a monolithic application and the size of the artifact (.exe) of Security Center is around 800Mb.

## Problem Statement ##

### Value Stream Mapping ###

Our first objective was to create a mapping of the full value stream (VSM), from ideation to delivery in production. By doing this we are able to identify bottlenecks and find the most pressing pain points.
The VSM took place during the first day and a half. While it may seem long, it is important not to underestimate the value of this exercise, as it is often the first time the team is exposed to a global view of the overall process, extending well outside their respective departments. It also leads to very interesting open discussions.

Here is the result of our value stream mapping activity.

*Figure 3: Value Stream Mapping*
![Figure 3: Value Stream Mapping](/images/genetec3.jpg)

*One difficulty during the VSM activity was that the process is constantly evolving and not really documented. We decided to use the most recent release as a base for the exercise.*

Based on the VSM, we estimated the total lead time (from ideation to deployed in production) to be around **8 months**, which was consistent with the previous releases.

Two areas are notably painful: Dev and QA, with around 3 months lead time each. The problem is a very common one: **large batch size**.

Features are developed on a separate branch, and will usually be a month long. Since multiple features will be delivered in a single release, the development team will usually work in a silo for several months, before deeming their work releasable, at which point a new release branch will be created, and the system test team will start the QA phase.

Since the changes after several months are so numerous, the test team has to check the integrity of the whole system, which takes a lot of time as well.

This creates a vicious circle: Since integration and testing are painful, our initial reflex is to do it the least possible, making them exponentially more painful.  A better approach is to strive to do it as often as possible, and incrementally work on improving the pipeline, **small batch size is key**.

On the technical level, the situation is made worse by the lack of automation: no continuous integration existed, and the automated test suite is unreliable, slow and triggered manually.

Deployments are triggered manually and use a custom tool to correctly configure the environment. It’s complicated for the team to deploy their solution on Azure just to test it.
Lastly, since no load test exists, release in production is a very long and critical process to ensure minimum risk for their customers: new releases are rolled out to an incrementally larger subset of their customer every week during a month.

The goal of the exercise was to identify pain points and shortcomings, but several points were very encouraging:

* Genetec’s people are very honest regarding the current state of things, and have a “can do” mindset. They acted as a real team to find possible counter-measure and did not spend any time blaming anyone. This is the kind of environment where a DevOps culture can really grow.
* While deployments need to be triggered manually, they are already well automated.
* Some unit tests are already in place, and the team understands the value of adding more in the future.
* Integration testing, while very long and unreliable, are automated, and the team is willing to improve their quality and performance.
* Functional tests are also already there, using a very interesting combination of Specflow and Selenium. Once again, they need to be triggered manually.
* A TFS 2015 update 2 is already used, and other teams at Genetec have a significant experience with it.

Based on this we decided to focus on bridging the existing parts together in an automated fashion with the goal of **reducing the feedback cycle**, and ultimately the lead time of development and quality assurance.


## Hackfest Focus ##

### Pipeline ###

When we engaged the conversation with Genetec about the backend application we understood that every step from development to production were manual. Each build was triggered manually from the TFS server and each step required manual operation with different actors to be executed.

However, they built some tools to help them and automated some parts like:

* “Stratocastor” which allows to prepare, deploy and monitor the cloud service deployment on Azure
* Wrote a bunch of automated tests
* Built some pieces of scripts on different steps

The thing is, nothing was there to orchestrate and manage all the steps in the pipeline from the simple check-in of code to the release on Azure with the tests on the tops of it!
That’s where we put the effort during the Hackfest. We started to explore the actual pipeline at the highest level.


*Figure 4: Pipeline in QA*
![Figure 4: Pipeline in QA](/images/genetec4.jpg)

We have these four steps, executed with different tools and any action between them, when one is done the next one is not triggered, the consequence could be some waiting time between two steps.
We also identified an important scrape/rate on the automated testing part, the tests had to be re-run few times to be sure everything was well executed.

One of the biggest bottleneck in terms of time is the “System testing”; it actually takes three months to execute them.

Following these statements, we decided to work on the Continuous Integration feature and see how we can implement it on Security Center.

### Continuous Integration ###

We decided to work on post build process of Security Center to not impact the current build process by creating a new one that will be executed after the current build.

*Figure 5: Post-Build step*

![Figure 5:Post-Build step](/images/genetec5.jpg)

In this new simple build, we prepared the package to be deployed automatically in the release. The goal was to eliminate the manual operation with the custom tool “Stratocastor” and deploy our application as often as we can on Azure automatically.

We quickly understood the pain of having an actual artifact of 800Mb, and the reality that we cannot build it every few minutes.

Here are the steps that we implemented in this short build:

* We ran some Powershell scripts to map the artifact folder of this new build to the previous one on the file share
* Copied the template service configuration and package files (.cscfg & .cspkg) in the artifact folder
* We also added 7 Unit tests in the process to test the feature, for the moment

*Figure 6: Build definition*

![Figure 6:Build definition](/images/genetec6.png)

David Chapdelaine, one of the developers, was quite satisfied to see the results live in TFS, and thought that being able to see the results directly in the build process report could encourage people to use and implement it. He also liked the “Code Coverage” feature, which for example allows to see the number of lines in the code, etc.

Here is an example of the Unit tests report during a build:

*Figure 7: Unit test results*

![Figure 7:Unit test results](/images/genetec7.png)

After having this new build, we also wanted to automatically execute it.
Francisco Davila, worked on a custom script that starts this new build when the previous one is done.

*Figure 8: New build connection*

![Figure 8:New build connection](/images/genetec8.png)

### Release Management and Continuous Delivery ###

The next big step and that was also new to the team was the Release: the availability to deploy automatically from a build to Azure and run all the tests.
Here are the steps in the release that we implemented:

*Figure 9: Unit test results*

![Figure 9:Unit test results](/images/genetec9.png)

From the previous build that we had just created, we have the .exe solution + the deployment files for the Cloud Service on Azure, which we call the “artifacts”.

Here are the main steps of our definition:

1.	The first step is to upload the .exe package somewhere accessible on Azure: a blob storage.
2.	We “tokenize” the deployment file (XML) thanks [Release Management Utility tasks] (https://marketplace.visualstudio.com/items?itemName=ms-devlabs.utilitytasks) extension
3.	We deploy our new cloud service thanks to the deployment file that we just customized one step before
    * When the Cloud Service is deployed on Azure, the virtual machine inside calls a bootstrap script (.CMD) that will download and install the .exe solution on it.
4.	We wait to have the “Callback” from the Cloud Services which indicates that the solution is installed inside the Cloud Services
5.	We copy some files mandatory to run our tests
6.	We run these tests to validate this new environment

We also add the possibility to destroy or not this environment at the end of the deployment.
Team Foundation Services, is now doing the same manual steps that “Stratocaster”, the custom tool, did but automatically.

Now, that we identified the process and the solution to deploy the package on Azure, this sequence can be run at each deployment if we want:

*Figure 10: Continuous Deployment*

![Figure 10:Continuous Deployment](/images/genetec10.png)

### Functional UI testing ###

As mentioned earlier, a comprehensive functional test suite already existed, but was triggered manually, and only during the QA phase.
In our pursuit of short feedback loops, making these tests run frequently and automatically is vital. We decided to integrate them into a new release definition.

These functional tests are written with SpecFlow (Which is Cucumber for .NET), which is an awesome tool for writing tests in plain language, making them readable by anyone, thus becoming a living documentation of the system.

Here is an example of a very basic SpecFlow test (unrelated to Genetec):  

{% highlight bash %}
Feature: US01 - Book Search
	As a potential customer
	I want to search for books by a simple phrase
	So that I can easily allocate books by something I remember from them.

Background:
	Given the following books
		|Author        |Title								
		|Martin Fowler |Analysis Patterns					
		|Eric Evans    |Domain Driven Design				

Scenario: Title should be matched
	When I search for books by the phrase 'Domain'
	Then the list of found books should contain only: 'Domain Driven Design'
{% endhighlight %}

And the implementation behind it (A lot of stuff has been omitted to fit in the page, this are just the interesting bits):

{% highlight csharp %}
public partial class US01_BookSearchFeature
{        
  public virtual void FeatureBackground()
  {
      TechTalk.SpecFlow.Table table1 = new TechTalk.SpecFlow.Table(new string[] {
                  "Author",
                  "Title"});
      table1.AddRow(new string[] {
                  "Martin Fowler",
                  "Analysis Patterns"});
      table1.AddRow(new string[] {
                  "Eric Evans",
                  "Domain Driven Design"});

      testRunner.Given("the following books", ((string)(null)), table1);
  }

  [TechTalk.SpecRun.ScenarioAttribute("Title should be matched")]
  public virtual void TitleShouldBeMatched()
  {
      TechTalk.SpecFlow.ScenarioInfo scenarioInfo = new TechTalk.SpecFlow.ScenarioInfo("Title should be matched", ((string[])(null)));
      testRunner.When("I search for books by the phrase \'Domain\'");
      testRunner.Then("the list of found books should contain only: \'Domain Driven Design\'");
  }

}

[Binding, Scope(Tag = "web")]
public class SearchSteps : SeleniumStepsBase
{
   [When(@"I search for books by the phrase '(.*)'")]
   public void WhenISearchForBooksByThePhrase(string searchTerm)
   {
       selenium.NavigateTo("Home");
       selenium.SetTextBoxValue("searchTerm", searchTerm);
       selenium.SubmitForm("searchForm");
   }

   [Then(@"the list of found books should contain only: '(.*)'")]
   public void ThenTheListOfFoundBooksShouldContainOnly(string expectedTitleList)
   {
       var foundBooks = selenium.FindElements(By.XPath("//table/tbody/tr"))
           .Select(row => new Book()
           {
               Title = row.FindElement(By.ClassName("title")).Text,
               Author = row.FindElement(By.ClassName("author")).Text,
           }).ToList();
       var expectedTitles = expectedTitleList.Split(',').Select(t => t.Trim().Trim('\''));

       BookAssertions.FoundBooksShouldMatchTitles(foundBooks, expectedTitles);
   }
}
{% endhighlight %}

At runtime, these tests use Selenium to interact with the UI, which requires a browser to run.
We decided to go with the simplest solution first: Create a custom build agent with a browser installed on it.  

*In the future, [PhantomJS](http://phantomjs.org/) could be used instead of a real browser. This would allow selenium tests to run even on a hosted agent, and would open the door for advanced scenarios like runnint multiple phantomjs instance in parallel, each executing a different subset of the functional tests.
[This blog post](https://blogs.msdn.microsoft.com/visualstudioalm/2016/01/27/getting-started-with-selenium-testing-in-a-continuous-integration-pipeline-with-visual-studio/) gives some hints on how to do that.*  

These tests need to pass a *.testsetting* file to vstest.exe to run correctly.
It turned out to be quite problematic as the VSTest task in Visual Studio Team Services didn’t  seem to accept it.

We opened an issue on [GitHub](https://github.com/Microsoft/vsts-tasks/issues/1618) , and quickly found the root of the issue as well as a workaround with the help of VSTS’s product team. A fix for this bug should be included in the next sprint!

After some struggling, here is the result of running these tests:

*Figure 11: Functional testing*

![Figure 11:Functional testing](/images/genetec11.png)

While there are many failed test, we were very pleased with the outcome! Most of the failures are due to incorrect configuration and not to an inherent problem with our approach. Everything should pass with a little bit of tweaking.

Currently, a run will clock at around three hours, which is too long to be part of a CI pipeline, but it shouldn’t be too hard to improve them with a little bit of ingenuity. In the meantime, these tests can either be run on a schedule (every night for example), or every X commits, which will already be a big improvement.

## Conclusion ##

This hackfest held the opportunity to have different people from different teams and departments. A good example is when the test team shared their methodology regarding the “System Testing” and explained the reason why it takes them three months of lead time to pass through it.
We identified some potential double work between the Back End and the Front End team on the testing part.

The reality is that both teams are using a kind of simulator for the cameras to test some scenarios like:

* Add a camera in the system
* Remove a camera
* Stream video from the camera
* 	…

This kind of scenarios was executed by both teams at each release. To remediate to this scenario, we decided to deploy an integration environment with the latest release of each part: Front End + Back End and run all of the tests automatically in this one.
Each team has its own lifecycle with different artifacts at the end. Every night, Genetec triggered a deployment, grabbed the latest artifacts on the both parts from each build process and deployed it in a common environment called “Integration” and ran all the tests on it.

*Figure 12: Integration Environment*

![Figure 12:Integration Environment](/images/genetec12.png)

Thanks to this process, every morning each team and each developer can see their work from the previous day in one common environment.
A good way to continuously integrate in production for Genetec.
Integrating DevOps practices into a several years old monolithic solution is never easy, but Genetec’s team really did a great job in the limited time we had. We were able to setup a skeleton for a full continuous delivery pipeline that will be built upon in the future.

Of course, implementing automation across all the pipeline and being able to continuously deliver value is a long journey, but the team had some great ideas for the future:

* Having a single Kanban board for the whole process, avoiding local optimizations.
* Splitting the monolith into several smaller independent components, that could each be released at a different rate;
* Finding the manual tests that are run every time and automate them, to reduce the lead time of the QA phase;
* Improve performance of integration and functional tests. Faster tests mean that they can be run more often: shorter feedback loops;
* The Hackfest allowed to engage conversations between the backend and frontend teams and to realize that some tests can be executed together and furthermore optimize the use of each team’s time;


## General lessons ##

One week is a very short time to change processes and practices. But the most important lesson Genetec learned is about working as a team.
They realized the importance and impact of cross-department communication:
* Avoid local optimizations
* Avoid duplicated work (for example, dev and test engineer both doing a same set of tests).
* Having a better knowledge of other departments processes, allowing anyone to pitch-in if needed.

While having Microsoft on site is certainly a catalyst for such events, organizations that want to improve should definitely consider taking some time to organize something similar.
In most companies, collaboration between departments is often kept to a minimum, each having separate objectives.
Hackfest like events are a unique way to break these silos and plant the seed for a DevOps mindset, helping your organization answer questions such as:
* How could we reduce our time to market?
* Could we automate some tasks to be more productive and reduce errors?
* Could we get actionable customer feedback earlier?

This kind of exercise is better known as a Kaizen event which is part of the [Kaizen movement (Continuous Improvement)](https://en.wikipedia.org/wiki/Kaizen).

## Resources ##
Links to additional resource (documentation, blog posts, github repos, ...)

* If you want to know more about running Selenium UI tests with SpecFlow, you can take a look at [this example on github](https://github.com/techtalk/SpecFlow-Examples/tree/master/ASP.NET-MVC/BookShop)
